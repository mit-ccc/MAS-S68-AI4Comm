
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1">
    
  <meta property="og:site_name" content="MIT MAS.S63" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="MIT MAS.S63!" />
  <meta property="og:description" content="Disussing Generative AI for Constructive Communications" />
  <meta property="og:url" content="http://www.mit.edu/~mas.s68/" />


  <title>MIT MAS.S68 | Generative AI for Constructive Communication</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">

  <!-- Google fonts -->
  <link href='http://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>

  <!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-MQGJWPF');</script>
  <!-- End Google Tag Manager -->

  <link rel="stylesheet" type="text/css" href="style.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

</head>

<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MQGJWPF"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

<!-- <script src="header.js"></script> -->
<!-- Navbar -->
<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand brand" href="index.html">MAS.S63 Home</a>
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="index.html#course">Course</a></li>
        <li><a href="index.html#logistics">Logistics</a></li>
        <li><a href="index.html#instructors">Instructors</a></li>
        <li><a href="index.html#schedule">Schedule</a></li>
      </ul>
    </div>
  </div>
</nav>

<!-- Header -->
<div id="header" style="text-align:center">
  <a href="https://www.ccc.mit.edu/">
        <img src="images/ccc-logo.png" class="logo-left">
  </a>
  <a href="https://web.mit.edu/">
    <img src="images/MIT_logo.png" class="logo-right">
  </a>
  <h1>MAS.S68: Generative AI for Constructive Communication <br><i>Evaluation and New Research Methods</i></h1>
  <h3>Spring 2023</h3>
  <!-- <img src="images/transformer.png" width="10%"> -->
  <div style="clear:both;"></div>
</div>


<!-- Intro -->
<div class="container sec" id="course">
  <h2>Purpose</h2>
    <p>
    The sudden accessibility of large language models have made seemingly impossible kinds of communication ubiquitous almost overnight. How do these models generate, affect, and streamline communication, and how can we better evaluate new research facilitated by these advances across fields? In particular, we wonder how LLM advances touch related research areas, including HCI, social sciences, behavior research, communications, journalism, and beyond.  
    </p>
    <p>
    What research potential is opened by these technological advances? What is unknown? Come be part of shaping a dynamic workshop + seminar on this rapidly changing topic!
    </p>

    <p> 
    This course will be a 6 unit seminar with a combination of speakers, paper discussions, research and design critiques, and research collaboration/workshops. 
    If you want to participate, fill out this <b><a href="https://docs.google.com/forms/d/e/1FAIpQLSdIPQXN3j7VwcYLR33XqpjmNeGP_5n0lsqqXHB81y2gUU3iSw/viewform">interest form</a></b> and come to the first class on Wednesday, 2/8.
    </p>
</div>

<div class="container sec" id="course-format">
    <h2>Course Format</h2>
      <p>
        This is a workshop and seminar, with an expected enrollment of 10-20 graduate students or advanced undergraduates. Students will divide their six weekly hours between readings, class participation, and project work. Students may come with a project already in progress, or team up with others to form a new project. In either case, the project should be focused on one of the main areas identified by the course: <b>evaluation, empirical studies, or new research methods</b> afforded by generative AI technologies. We will come together to share and critique projects through the semester, culminating in final presentations.
      </p>
</div>

<!-- Logistics -->
<div class="container sec" id="logistics">
  <h2>Logistics</h2>
  <ul>
    <li><b>Classes</b> are in person on Wednesdays from 2:00PM - 4:00PM Eastern Standard Time in E15-359</li>
    <li><b>Prerequisites</b>: None; instructor permission required.</li>
    <li><b>Grading</b>: 6 credits. Evaluation based on participation in reading group and design critiques. Final project will include a mid-semester lit review followed by a final research proposal. Students may also do a research project by the end of the course, depending on their preferred pace.</li>
    <!-- <li><b>Attendance Policy</b>: Every class, submit a response to our <a href="https://docs.google.com/forms/d/e/1FAIpQLScTXdhZEd_NR8Z0oUFnQ7T2BaLcqi3DWEOtE8iTAKwVWjuFAQ/viewform?usp=sf_link">Google Form</a></li> -->
    <!-- <li><b>Discord</b>: <a href="https://discord.gg/2vE7gbsjzA">Link</a> </li> -->
    <li><b>Contact</b>: If you have any questions about the course, contact us at <a href="mailto:ai-ccc@mit.edu">ai-ccc@mit.edu</a>
  </ul>
  <br>
</div>

<!-- Staff Info -->
<div class="sechighlight">
  <div class="container sec" id="instructors">
    <!-- <div class="row"> -->
    <div class="col-md-7">
      <h3>Instructors</h3>

      <div class="instructor">
        <a target="_blank" rel="noopener noreferrer" href="https://www.ccc.mit.edu/person/doug-beeferman/">
        <div class="instructorphoto"><img src="images/Doug.jpeg"></div>
        <div>Doug Beeferman</div>
        </a>
      </div>

      <div class="instructor">
        <a target = "_blank" rel="noopener noreferrer" href="https://www.mit.edu/~jkabbara/">
        <div class="instructorphoto"><img src="images/Jad.jpeg"></div>
        <div>Jad Kabbara</div>
        </a>
      </div>

      <div class="instructor">
        <a target="_blank" rel="noopener noreferrer" href="https://www.ccc.mit.edu/person/suyash-fulay/">
        <div class="instructorphoto"><img src="images/Suyash.jpeg"></div>
        <div>Suyash Fulay</div>
        </a>
      </div>

      <div class="instructor">
        <a target = "_blank" rel="noopener noreferrer" href="https://www.shaynelongpre.com/">
        <div class="instructorphoto"><img src="images/Shayne.png"></div>
        <div>Shayne Longpre</div>
        </a>
      </div>

      <div class="instructor">
        <a target = "_blank" rel="noopener noreferrer" href="https://www.mit.edu/~hjian42/">
        <div class="instructorphoto"><img src="images/Hang.png"></div>
        <div>Hang Jiang</div>
        </a>
      </div>

      <div class="instructor">
        <a target = "_blank" rel="noopener noreferrer" href="https://www.ccc.mit.edu/person/hope-schroeder/">
        <div class="instructorphoto"><img src="images/Hope.jpeg"></div>
        <div>Hope Schroeder</div>
        </a>
      </div>
    
    </div>
    <div class="col-md-3">
      <h3>Faculty Advisor</h3>
      <div class="instructor">
        <a rel="noopener noreferrer" target="_blank" href="https://www.ccc.mit.edu/person/deb-roy/">
        <div class="instructorphoto"><img src="images/deb.jpeg"></div>
        <div>Deb K. Roy</div>
        </a>
      </div>
    </div>
  </div> 

  </div>


<!-- </div> -->
<!-- Note the margin-top:-20px and the <br> serve to make the #schedule hyperlink display correctly (with the h2 header visible) -->
<div class="container sec" id="schedule" style="margin-top:-20px">

<h2>Schedule</h2>

  <p> The current class schedule is below (subject to change) </p>
  <br>

<table class="table">
  <colgroup>
    <col style="width:20%">
    <col style="width:40%">
    <col style="width:40%">
        
  </colgroup>
  <thead>
  <tr class="active">
    <th>Date</th>
    <th>Description</th>
    <th>Course Materials</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>Feb 8</td>
    <td>Introduction to Transformers<br>
      Speaker:
      <i>
        <a href="https://scholar.google.com/citations?user=l8WuQJgAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer"><strong>Andrej Karpathy</strong></a>
      </i>
    </td>
    <td>
      Recommended Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All You Need</a></li>
        <li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
        <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/"> The Annotated Transformer</a> </li>
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Feb 15</td>
    <td>
      Language and Human Alignment<br>
      Speaker:
      <i><a href="https://scholar.google.com/citations?hl=en&user=beiWcokAAAAJ" target="_blank" rel="noopener noreferrer"><strong>Jan Leike</strong> (OpenAI)</a></i>
    </td>
    <td>
      Recommended Readings:
      <ol>
        <li><a href="https://openai.com/blog/chatgpt/" target="_blank" rel="noopener noreferrer">ChatGPT</a></li>
        <li><a href="https://openai.com/blog/instruction-following/" target="_blank" rel="noopener noreferrer">InstructGPT</a></li>
        <li><a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">Language Models are Few-Shot Learners (GPT-3)</a></li>
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Feb 22</td>
    <td>
      Emergent Abilities and Scaling in LLMs<br>
      Speaker: <i><a href="https://scholar.google.com/citations?user=wA5TK_0AAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer"><strong>Jason Wei</strong> (Google Brain)</a></i>
    </td>
    <td>
      Recommended Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/2206.07682" target="_blank" rel="noopener noreferrer">Emergent Abilities of Large Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener noreferrer">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></li>
        <li><a href="https://arxiv.org/abs/2210.11416" target="_blank" rel="noopener noreferrer">Scaling Instruction-Finetuned Language Models</a></li>
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Mar 1</td>
    <td>
      Strategic Games<br>
      Speaker:
            <i><a href="https://scholar.google.com/citations?hl=en&user=RLDbLcUAAAAJ" target="_blank" rel="noopener noreferrer"><strong>Noam Brown</strong> (FAIR)</a></i>
    </td>
    <td>
      Recommended Readings:
      <ol>
                <li><a href="https://www.science.org/doi/full/10.1126/science.ade9097" target="_blank" rel="noopener noreferrer">Human-level play in the game of Diplomacy by combining language models with strategic reasoning</a></li>
                <li><a href="https://proceedings.mlr.press/v162/jacob22a.html" target="_blank" rel="noopener noreferrer">Modeling Strong and Human-Like Gameplay with KL-Regularized Search</a></li>
                <li><a href="https://proceedings.neurips.cc/paper/2021/hash/95f2b84de5660ddf45c8a34933a2e66f-Abstract.html" target="_blank" rel="noopener noreferrer">No-Press Diplomacy from Scratch</a></li>
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Mar 8</td>
    <td>
      Robotics and Imitation Learning<br>
      Speaker:
      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>
    </td>
    <td>
      Recommended Readings:
      <ol>
        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>
        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>
        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Mar 15</td>
    <td>
      Common Sense Reasoning<br>
      Speaker:
            <i><a href="https://scholar.google.com/citations?user=vhP-tlcAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer"><strong>Yejin Choi</strong> (U. Washington / Allen Institute for AI)</a></i>
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Mar 22</td>
    <td>
<!--      Reasoning and Emergence in LLMs<br>-->
<!--      Speaker: <i><a href="https://scholar.google.com/citations?user=wA5TK_0AAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer"><strong>Jason Wei</strong> (Google Brain)</a></i>-->
      TBA
    </td>
    <td>
      Recommended Readings:
<!--      <ol>-->
<!--                <li><a href="https://arxiv.org/abs/2206.07682" target="_blank" rel="noopener noreferrer">Emergent Abilities of Large Language Models</a></li>-->
<!--                <li><a href="https://arxiv.org/abs/2201.11903" target="_blank" rel="noopener noreferrer">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></li>-->
<!--                <li><a href="https://arxiv.org/abs/2109.01652" target="_blank" rel="noopener noreferrer">Finetuned Language Models Are Zero-Shot Learners</a></li>-->
<!--      </ol>-->

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Mar 29</td>
    <td>
      In-Context Learning & Faithful Reasoning<br>
      Speakers:
            <i><a href="https://scholar.google.com/citations?user=bXOt49QAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer"><strong>Stephanie Chan</strong> (DeepMind)</a></i>
            & <i><a href="https://scholar.google.com/citations?user=faiFBhoAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener noreferrer"><strong>Antonia Creswell</strong> (DeepMind)</a></i>
    </td>
    <td>
      Recommended Readings:
      <ol>
                <li><a href="https://arxiv.org/abs/2205.05055" target="_blank" rel="noopener noreferrer">Data Distributional Properties Drive Emergent In-Context Learning in Transformers</a></li>
                <li><a href="https://arxiv.org/abs/2208.14271" target="_blank" rel="noopener noreferrer">Faithful Reasoning Using Large Language Models</a></li>
                <li><a href="https://arxiv.org/abs/2207.07051" target="_blank" rel="noopener noreferrer">Language models show human-like content effects on reasoning</a></li>
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Apr 5</td>
    <td>
      Neuroscience<br>
      Speaker: TBA
      <!--      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>-->
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Apr 12</td>
    <td>
      Wrap Up<br>
      Speaker: TBA
      <!--      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>-->
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>


  <tr>
    <td>Apr 19</td>
    <td>
      Wrap Up<br>
      Speaker: TBA
      <!--      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>-->
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>Apr 26</td>
    <td>
      Wrap Up<br>
      Speaker: TBA
      <!--      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>-->
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>May 3</td>
    <td>
      Wrap Up<br>
      Speaker: TBA
      <!--      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>-->
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  <tr>
    <td>May 10</td>
    <td>
      Wrap Up<br>
      Speaker: TBA
      <!--      <i><a href="https://scholar.google.com/citations?user=LIJQ_ZYAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"><strong>Ted Xiao</strong> (Google Brain)</a></i>-->
    </td>
    <td>
      Recommended Readings:
      <ol>
        <!--        <li><a href="https://arxiv.org/abs/2212.06817" target="_blank" rel="noopener noreferrer">RT-1: Robotics Transformer for Real-World Control at Scale</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2204.01691" target="_blank" rel="noopener noreferrer">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a></li>-->
        <!--        <li><a href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer">Inner Monologue: Embodied Reasoning through Planning with Language Models</a></li>-->
      </ol>

      Additional Readings:
      <ol>
      </ol>
    </td>
  </tr>

  </tbody>
</table>
</div>

<div class="container">
    <hr>
    <footer>
        <p>Template credit to <a href="https://web.stanford.edu/class/cs25/" target="_blank">CS25 at Stanford</a>.</p>
    </footer>
</div>

<!-- jQuery and Bootstrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
